{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import required packages\n",
    "#basics\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "#misc\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "#stats\n",
    "#from scipy.misc import imread\n",
    "from scipy import sparse\n",
    "import scipy.stats as ss\n",
    "\n",
    "#viz\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud ,STOPWORDS\n",
    "from PIL import Image\n",
    "#import matplotlib_venn as venn\n",
    "\n",
    "#nlp\n",
    "import string\n",
    "import re    #for regex\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#import spacy\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tweet tokenizer does not split at apostophes which is what we want\n",
    "from nltk.tokenize import TweetTokenizer   \n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "#FeatureEngineering\n",
    "#!pip install lightgbm\n",
    "#from lightgbm import LGBMClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, decomposition, ensemble\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import  textblob\n",
    "#import xgboost\n",
    "#from keras.preprocessing import text, sequence\n",
    "#from keras import layers, models, optimizers\n",
    "\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "from textblob import Word\n",
    "\n",
    "#settings\n",
    "start_time=time.time()\n",
    "color = sns.color_palette()\n",
    "sns.set_style(\"dark\")\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "tokenizer=TweetTokenizer()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read Bank.csv into a DataFrame\n",
    "Bank = pd.read_excel('C:/Users/Yash/Desktop/STATS/CASE STUDY FOR ML/5. Bank Reviews-Complaints Analysis/BankReviews.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Stars</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>BankName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>5</td>\n",
       "      <td>Great job, Wyndham Capital! Each person was pr...</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-02-10</td>\n",
       "      <td>5</td>\n",
       "      <td>Matthew Richardson is professional and helpful...</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-21</td>\n",
       "      <td>5</td>\n",
       "      <td>We had a past experience with Wyndham Mortgage...</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-17</td>\n",
       "      <td>5</td>\n",
       "      <td>We have been dealing with Brad Thomka from the...</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-05-27</td>\n",
       "      <td>5</td>\n",
       "      <td>I can't express how grateful I am for the supp...</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Stars                                            Reviews  \\\n",
       "0 2017-04-10      5  Great job, Wyndham Capital! Each person was pr...   \n",
       "1 2017-02-10      5  Matthew Richardson is professional and helpful...   \n",
       "2 2017-08-21      5  We had a past experience with Wyndham Mortgage...   \n",
       "3 2017-12-17      5  We have been dealing with Brad Thomka from the...   \n",
       "4 2016-05-27      5  I can't express how grateful I am for the supp...   \n",
       "\n",
       "                   BankName  \n",
       "0  Wyndham Capital Mortgage  \n",
       "1  Wyndham Capital Mortgage  \n",
       "2  Wyndham Capital Mortgage  \n",
       "3  Wyndham Capital Mortgage  \n",
       "4  Wyndham Capital Mortgage  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bank.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bank=Bank[['Stars', 'Reviews',  'BankName']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stars</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>BankName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Great job, Wyndham Capital! Each person was pr...</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Matthew Richardson is professional and helpful...</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>We had a past experience with Wyndham Mortgage...</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>We have been dealing with Brad Thomka from the...</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>I can't express how grateful I am for the supp...</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stars                                            Reviews  \\\n",
       "0      5  Great job, Wyndham Capital! Each person was pr...   \n",
       "1      5  Matthew Richardson is professional and helpful...   \n",
       "2      5  We had a past experience with Wyndham Mortgage...   \n",
       "3      5  We have been dealing with Brad Thomka from the...   \n",
       "4      5  I can't express how grateful I am for the supp...   \n",
       "\n",
       "                   BankName  \n",
       "0  Wyndham Capital Mortgage  \n",
       "1  Wyndham Capital Mortgage  \n",
       "2  Wyndham Capital Mortgage  \n",
       "3  Wyndham Capital Mortgage  \n",
       "4  Wyndham Capital Mortgage  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Bank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: It may take some time to process the function if the data is huge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Reviews'] = df['Reviews'].astype(str)\n",
    "df['count_sent']=df[\"Reviews\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n",
    "\n",
    "#Word count in each comment:\n",
    "df['count_word']=df[\"Reviews\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "#Unique word count\n",
    "df['count_unique_word']=df[\"Reviews\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "#Letter count\n",
    "df['count_letters']=df[\"Reviews\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "#Word density\n",
    "\n",
    "df['word_density'] = df['count_letters'] / (df['count_word']+1)\n",
    "\n",
    "#punctuation count\n",
    "df[\"count_punctuations\"] =df[\"Reviews\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "#upper case words count\n",
    "df[\"count_words_upper\"] = df[\"Reviews\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "#upper case words count\n",
    "df[\"count_words_lower\"] = df[\"Reviews\"].apply(lambda x: len([w for w in str(x).split() if w.islower()]))\n",
    "\n",
    "#title case words count\n",
    "df[\"count_words_title\"] = df[\"Reviews\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "#Number of stopwords\n",
    "df[\"count_stopwords\"] = df[\"Reviews\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "#Average length of the words\n",
    "df[\"mean_word_len\"] = df[\"Reviews\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "#Number of numeric\n",
    "df['numeric'] = df['Reviews'].apply(lambda x :len([x for x in x.split() if x.isdigit()]))\n",
    "\n",
    "#Number of alphanumeric\n",
    "df['alphanumeric'] = df['Reviews'].apply(lambda x :len([x for x in x.split() if x.isalnum()]))\n",
    "\n",
    "#Number of alphabetics\n",
    "df['alphabetetics'] = df['Reviews'].apply(lambda x :len([x for x in x.split() if x.isalpha()]))\n",
    "\n",
    "#Number of alphabetics\n",
    "df['Spaces'] = df['Reviews'].apply(lambda x :len([x for x in x.split() if x.isspace()]))\n",
    "\n",
    "#Number of Words ends with\n",
    "df['words_ends_with_et'] = df['Reviews'].apply(lambda x :len([x for x in x.lower().split() if x.endswith('et')]))\n",
    "\n",
    "#Number of Words ends with\n",
    "df['words_start_with_no'] = df['Reviews'].apply(lambda x :len([x for x in x.lower().split() if x.startswith('no')]))\n",
    "\n",
    "# Count the occurences of all words\n",
    "df['wordcounts'] = df['Reviews'].apply(lambda x :dict([ [t, x.split().count(t)] for t in set(x.split()) ]))\n",
    "\n",
    "pos_family = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}\n",
    "\n",
    "# function to check and get the part of speech tag count of a words in a given sentence\n",
    "def check_pos_tag(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_family[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt\n",
    "\n",
    "df['noun_count'] = df['Reviews'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "df['verb_count'] = df['Reviews'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "df['adj_count']  = df['Reviews'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "df['adv_count']  = df['Reviews'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "df['pron_count'] = df['Reviews'].apply(lambda x: check_pos_tag(x, 'pron')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Sentiment analysis using Textblob module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment.polarity ---will tell polarity is >0 means positive(accept) <0 is negitive(reject)\n",
    "\n",
    "# Textblob has al ready the negitive and postive PHRASES to use thats how polarity is calculated\n",
    "\n",
    "df['sentiment'] = df[\"Reviews\"].apply(lambda x: TextBlob(x).sentiment.polarity )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    410\n",
       "1     95\n",
       "Name: Stars, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bank.Stars.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(378,)\n",
      "(127,)\n",
      "(378,)\n",
      "(127,)\n"
     ]
    }
   ],
   "source": [
    "# create a new DataFrame that only contains the 5-star and 1-star reviews\n",
    "#Bank_best_worst = Bank[(Bank.stars==5) | (Bank.stars==1)]\n",
    "\n",
    "# define X and y\n",
    "X = Bank.Reviews\n",
    "y = Bank.Stars\n",
    "\n",
    "# split the new DataFrame into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(505, 27)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bank.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stars</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>BankName</th>\n",
       "      <th>count_sent</th>\n",
       "      <th>count_word</th>\n",
       "      <th>count_unique_word</th>\n",
       "      <th>count_letters</th>\n",
       "      <th>word_density</th>\n",
       "      <th>count_punctuations</th>\n",
       "      <th>count_words_upper</th>\n",
       "      <th>...</th>\n",
       "      <th>Spaces</th>\n",
       "      <th>words_ends_with_et</th>\n",
       "      <th>words_start_with_no</th>\n",
       "      <th>wordcounts</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>pron_count</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Great job, Wyndham Capital! Each person was pr...</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>126</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'you!': 1, 'and': 1, 'smoothly.': 1, 'through...</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Matthew Richardson is professional and helpful...</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>159</td>\n",
       "      <td>6.115385</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'service,': 1, 'and': 1, 'much': 1, 'He': 1, ...</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.453333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>We had a past experience with Wyndham Mortgage...</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>64</td>\n",
       "      <td>462</td>\n",
       "      <td>5.775000</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'only': 1, 'Wyndham!!': 1, 'and': 4, 'would':...</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.033231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>We have been dealing with Brad Thomka from the...</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "      <td>1</td>\n",
       "      <td>108</td>\n",
       "      <td>78</td>\n",
       "      <td>605</td>\n",
       "      <td>5.550459</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'pulled': 1, 'found': 1, 'ultimately': 1, 'pr...</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>0.093740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>I can't express how grateful I am for the supp...</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>47</td>\n",
       "      <td>341</td>\n",
       "      <td>5.683333</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'service,': 1, 'and': 4, 'I': 3, 'took': 1, '...</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stars                                            Reviews  \\\n",
       "0      5  Great job, Wyndham Capital! Each person was pr...   \n",
       "1      5  Matthew Richardson is professional and helpful...   \n",
       "2      5  We had a past experience with Wyndham Mortgage...   \n",
       "3      5  We have been dealing with Brad Thomka from the...   \n",
       "4      5  I can't express how grateful I am for the supp...   \n",
       "\n",
       "                   BankName  count_sent  count_word  count_unique_word  \\\n",
       "0  Wyndham Capital Mortgage           1          19                 19   \n",
       "1  Wyndham Capital Mortgage           1          25                 23   \n",
       "2  Wyndham Capital Mortgage           1          79                 64   \n",
       "3  Wyndham Capital Mortgage           1         108                 78   \n",
       "4  Wyndham Capital Mortgage           1          59                 47   \n",
       "\n",
       "   count_letters  word_density  count_punctuations  count_words_upper  ...  \\\n",
       "0            126      6.300000                   4                  0  ...   \n",
       "1            159      6.115385                   4                  0  ...   \n",
       "2            462      5.775000                   8                  0  ...   \n",
       "3            605      5.550459                   9                  0  ...   \n",
       "4            341      5.683333                   6                  3  ...   \n",
       "\n",
       "   Spaces  words_ends_with_et  words_start_with_no  \\\n",
       "0       0                   0                    0   \n",
       "1       0                   0                    0   \n",
       "2       0                   0                    1   \n",
       "3       0                   0                    1   \n",
       "4       0                   0                    1   \n",
       "\n",
       "                                          wordcounts  noun_count  verb_count  \\\n",
       "0  {'you!': 1, 'and': 1, 'smoothly.': 1, 'through...           8           3   \n",
       "1  {'service,': 1, 'and': 1, 'much': 1, 'He': 1, ...           7           3   \n",
       "2  {'only': 1, 'Wyndham!!': 1, 'and': 4, 'would':...          24          10   \n",
       "3  {'pulled': 1, 'found': 1, 'ultimately': 1, 'pr...          30          17   \n",
       "4  {'service,': 1, 'and': 4, 'I': 3, 'took': 1, '...          16           9   \n",
       "\n",
       "   adj_count  adv_count  pron_count  sentiment  \n",
       "0          1          1           3   0.533333  \n",
       "1          4          2           4   0.453333  \n",
       "2          5          8           8  -0.033231  \n",
       "3          8          7          11   0.093740  \n",
       "4          4          4           8   0.125000  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Stars</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reviews</th>\n",
       "      <td>Great job, Wyndham Capital! Each person was pr...</td>\n",
       "      <td>Matthew Richardson is professional and helpful...</td>\n",
       "      <td>We had a past experience with Wyndham Mortgage...</td>\n",
       "      <td>We have been dealing with Brad Thomka from the...</td>\n",
       "      <td>I can't express how grateful I am for the supp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BankName</th>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_sent</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_word</th>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>79</td>\n",
       "      <td>108</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_unique_word</th>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>64</td>\n",
       "      <td>78</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_letters</th>\n",
       "      <td>126</td>\n",
       "      <td>159</td>\n",
       "      <td>462</td>\n",
       "      <td>605</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_density</th>\n",
       "      <td>6.3</td>\n",
       "      <td>6.11538</td>\n",
       "      <td>5.775</td>\n",
       "      <td>5.55046</td>\n",
       "      <td>5.68333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_punctuations</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_words_upper</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_words_lower</th>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>88</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_words_title</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_stopwords</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>49</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_word_len</th>\n",
       "      <td>5.68421</td>\n",
       "      <td>5.4</td>\n",
       "      <td>4.86076</td>\n",
       "      <td>4.61111</td>\n",
       "      <td>4.79661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numeric</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alphanumeric</th>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>73</td>\n",
       "      <td>101</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alphabetetics</th>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>73</td>\n",
       "      <td>101</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spaces</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>words_ends_with_et</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>words_start_with_no</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wordcounts</th>\n",
       "      <td>{'you!': 1, 'and': 1, 'smoothly.': 1, 'through...</td>\n",
       "      <td>{'service,': 1, 'and': 1, 'much': 1, 'He': 1, ...</td>\n",
       "      <td>{'only': 1, 'Wyndham!!': 1, 'and': 4, 'would':...</td>\n",
       "      <td>{'pulled': 1, 'found': 1, 'ultimately': 1, 'pr...</td>\n",
       "      <td>{'service,': 1, 'and': 4, 'I': 3, 'took': 1, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>noun_count</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verb_count</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adj_count</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adv_count</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pron_count</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.453333</td>\n",
       "      <td>-0.0332308</td>\n",
       "      <td>0.0937401</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     0  \\\n",
       "Stars                                                                5   \n",
       "Reviews              Great job, Wyndham Capital! Each person was pr...   \n",
       "BankName                                      Wyndham Capital Mortgage   \n",
       "count_sent                                                           1   \n",
       "count_word                                                          19   \n",
       "count_unique_word                                                   19   \n",
       "count_letters                                                      126   \n",
       "word_density                                                       6.3   \n",
       "count_punctuations                                                   4   \n",
       "count_words_upper                                                    0   \n",
       "count_words_lower                                                   14   \n",
       "count_words_title                                                    5   \n",
       "count_stopwords                                                      5   \n",
       "mean_word_len                                                  5.68421   \n",
       "numeric                                                              0   \n",
       "alphanumeric                                                        15   \n",
       "alphabetetics                                                       15   \n",
       "Spaces                                                               0   \n",
       "words_ends_with_et                                                   0   \n",
       "words_start_with_no                                                  0   \n",
       "wordcounts           {'you!': 1, 'and': 1, 'smoothly.': 1, 'through...   \n",
       "noun_count                                                           8   \n",
       "verb_count                                                           3   \n",
       "adj_count                                                            1   \n",
       "adv_count                                                            1   \n",
       "pron_count                                                           3   \n",
       "sentiment                                                     0.533333   \n",
       "\n",
       "                                                                     1  \\\n",
       "Stars                                                                5   \n",
       "Reviews              Matthew Richardson is professional and helpful...   \n",
       "BankName                                      Wyndham Capital Mortgage   \n",
       "count_sent                                                           1   \n",
       "count_word                                                          25   \n",
       "count_unique_word                                                   23   \n",
       "count_letters                                                      159   \n",
       "word_density                                                   6.11538   \n",
       "count_punctuations                                                   4   \n",
       "count_words_upper                                                    0   \n",
       "count_words_lower                                                   20   \n",
       "count_words_title                                                    5   \n",
       "count_stopwords                                                     10   \n",
       "mean_word_len                                                      5.4   \n",
       "numeric                                                              0   \n",
       "alphanumeric                                                        21   \n",
       "alphabetetics                                                       21   \n",
       "Spaces                                                               0   \n",
       "words_ends_with_et                                                   0   \n",
       "words_start_with_no                                                  0   \n",
       "wordcounts           {'service,': 1, 'and': 1, 'much': 1, 'He': 1, ...   \n",
       "noun_count                                                           7   \n",
       "verb_count                                                           3   \n",
       "adj_count                                                            4   \n",
       "adv_count                                                            2   \n",
       "pron_count                                                           4   \n",
       "sentiment                                                     0.453333   \n",
       "\n",
       "                                                                     2  \\\n",
       "Stars                                                                5   \n",
       "Reviews              We had a past experience with Wyndham Mortgage...   \n",
       "BankName                                      Wyndham Capital Mortgage   \n",
       "count_sent                                                           1   \n",
       "count_word                                                          79   \n",
       "count_unique_word                                                   64   \n",
       "count_letters                                                      462   \n",
       "word_density                                                     5.775   \n",
       "count_punctuations                                                   8   \n",
       "count_words_upper                                                    0   \n",
       "count_words_lower                                                   64   \n",
       "count_words_title                                                   14   \n",
       "count_stopwords                                                     31   \n",
       "mean_word_len                                                  4.86076   \n",
       "numeric                                                              0   \n",
       "alphanumeric                                                        73   \n",
       "alphabetetics                                                       73   \n",
       "Spaces                                                               0   \n",
       "words_ends_with_et                                                   0   \n",
       "words_start_with_no                                                  1   \n",
       "wordcounts           {'only': 1, 'Wyndham!!': 1, 'and': 4, 'would':...   \n",
       "noun_count                                                          24   \n",
       "verb_count                                                          10   \n",
       "adj_count                                                            5   \n",
       "adv_count                                                            8   \n",
       "pron_count                                                           8   \n",
       "sentiment                                                   -0.0332308   \n",
       "\n",
       "                                                                     3  \\\n",
       "Stars                                                                5   \n",
       "Reviews              We have been dealing with Brad Thomka from the...   \n",
       "BankName                                      Wyndham Capital Mortgage   \n",
       "count_sent                                                           1   \n",
       "count_word                                                         108   \n",
       "count_unique_word                                                   78   \n",
       "count_letters                                                      605   \n",
       "word_density                                                   5.55046   \n",
       "count_punctuations                                                   9   \n",
       "count_words_upper                                                    0   \n",
       "count_words_lower                                                   88   \n",
       "count_words_title                                                   18   \n",
       "count_stopwords                                                     49   \n",
       "mean_word_len                                                  4.61111   \n",
       "numeric                                                              0   \n",
       "alphanumeric                                                       101   \n",
       "alphabetetics                                                      101   \n",
       "Spaces                                                               0   \n",
       "words_ends_with_et                                                   0   \n",
       "words_start_with_no                                                  1   \n",
       "wordcounts           {'pulled': 1, 'found': 1, 'ultimately': 1, 'pr...   \n",
       "noun_count                                                          30   \n",
       "verb_count                                                          17   \n",
       "adj_count                                                            8   \n",
       "adv_count                                                            7   \n",
       "pron_count                                                          11   \n",
       "sentiment                                                    0.0937401   \n",
       "\n",
       "                                                                     4  \n",
       "Stars                                                                5  \n",
       "Reviews              I can't express how grateful I am for the supp...  \n",
       "BankName                                      Wyndham Capital Mortgage  \n",
       "count_sent                                                           1  \n",
       "count_word                                                          59  \n",
       "count_unique_word                                                   47  \n",
       "count_letters                                                      341  \n",
       "word_density                                                   5.68333  \n",
       "count_punctuations                                                   6  \n",
       "count_words_upper                                                    3  \n",
       "count_words_lower                                                   50  \n",
       "count_words_title                                                    9  \n",
       "count_stopwords                                                     30  \n",
       "mean_word_len                                                  4.79661  \n",
       "numeric                                                              0  \n",
       "alphanumeric                                                        53  \n",
       "alphabetetics                                                       53  \n",
       "Spaces                                                               0  \n",
       "words_ends_with_et                                                   0  \n",
       "words_start_with_no                                                  1  \n",
       "wordcounts           {'service,': 1, 'and': 4, 'I': 3, 'took': 1, '...  \n",
       "noun_count                                                          16  \n",
       "verb_count                                                           9  \n",
       "adj_count                                                            4  \n",
       "adv_count                                                            4  \n",
       "pron_count                                                           8  \n",
       "sentiment                                                        0.125  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_best_worst = Bank[(Bank.Stars==5) | (Bank.Stars==1)]\n",
    "bank_best_worst.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating user defined functions for clean the text and pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abbrevations and Words correction\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:{}`+=~|.!?,'0-9]\", \"\", text)\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "nltk.corpus.stopwords.words('english') #tells the stop word in english\n",
    "\n",
    "#nltk.corpus.stopwords.words('french') #tells the stop word in french ETC ETC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def pre_process(text):\n",
    "    #text = text.str.replace('/','')                           #Replacing the / with none\n",
    "    #text = text.apply(lambda x: re.sub(\"  \",\" \", x))          #Replacing double space with single space\n",
    "    #text = re.sub(r\"[-()\\\"#/@;:{}`+=~|.!?,']\", \"\", text)      #Replacing special character with none\n",
    "    #text = re.sub(r'[0-9]+', '', text)                        #Replacing numbers with none\n",
    "    #text = text.apply(lambda x: \" \".join(x.translate(str.maketrans('', '', string.punctuation)) for x in x.split() if x.isalpha()))\n",
    "    text = text.apply(lambda x: \" \".join(x for x in x.split() if x not in stop)) #Removing stop words\n",
    "    #text = text.apply(lambda x: str(TextBlob(x).correct()))                      #Correct spelling corrections\n",
    "    #text = text.apply(lambda x: \" \".join(PorterStemmer().stem(word) for word in x.split())) #Stemming using porter stemmer\n",
    "    #text = text.apply(lambda x: \" \".join(stemmer_func(word) for word in x.split()))        #Stemming\n",
    "    #text = text.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))   #lemmatization\n",
    "    #text = text.apply(lambda x: \" \".join(word for word, pos in pos_tag(x.split()) if pos not in ['NN','NNS','NNP','NNPS'])) #Removing nouns etc\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(lambda x: clean_text(x))\n",
    "X_test = X_test.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pre_process(X_train)\n",
    "X_test=pre_process(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Vectorization (Count, Tfidf, Hashing)\n",
    "        - Charter level\n",
    "        - Word level\n",
    "        - n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
    "                             ngram_range=(1, 1 ), max_df=0.9,\n",
    "                             min_df=0.01, \n",
    "                             encoding='latin-1' ,\n",
    "                             max_features=800)\n",
    "xtrain_count = count_vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<378x732 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9755 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the document term metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm=xtrain_count.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaron',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'absolutely',\n",
       " 'accept',\n",
       " 'accommodating',\n",
       " 'account',\n",
       " 'accurate',\n",
       " 'achieve',\n",
       " 'across',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'adam',\n",
       " 'adan',\n",
       " 'additional',\n",
       " 'advice',\n",
       " 'agent',\n",
       " 'agreed',\n",
       " 'alex',\n",
       " 'almost',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'always',\n",
       " 'amazing',\n",
       " 'american',\n",
       " 'amount',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answering',\n",
       " 'answers',\n",
       " 'anthony',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'application',\n",
       " 'apply',\n",
       " 'appraisal',\n",
       " 'appraiser',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'approved',\n",
       " 'around',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'aspects',\n",
       " 'asset',\n",
       " 'attention',\n",
       " 'attentive',\n",
       " 'attorney',\n",
       " 'available',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'back',\n",
       " 'bad',\n",
       " 'balance',\n",
       " 'bank',\n",
       " 'banks',\n",
       " 'barrett',\n",
       " 'based',\n",
       " 'basis',\n",
       " 'beat',\n",
       " 'became',\n",
       " 'began',\n",
       " 'beginning',\n",
       " 'believe',\n",
       " 'beneficial',\n",
       " 'best',\n",
       " 'better',\n",
       " 'beyond',\n",
       " 'big',\n",
       " 'bob',\n",
       " 'brent',\n",
       " 'broker',\n",
       " 'brokers',\n",
       " 'bumps',\n",
       " 'business',\n",
       " 'buy',\n",
       " 'buyer',\n",
       " 'buyers',\n",
       " 'buying',\n",
       " 'ca',\n",
       " 'call',\n",
       " 'called',\n",
       " 'calling',\n",
       " 'calls',\n",
       " 'calm',\n",
       " 'came',\n",
       " 'cannot',\n",
       " 'cant',\n",
       " 'capital',\n",
       " 'care',\n",
       " 'carrion',\n",
       " 'caused',\n",
       " 'causing',\n",
       " 'certainly',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'changes',\n",
       " 'charge',\n",
       " 'check',\n",
       " 'chose',\n",
       " 'chris',\n",
       " 'circumstances',\n",
       " 'clear',\n",
       " 'clients',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'closing',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'comfortable',\n",
       " 'commented',\n",
       " 'commission',\n",
       " 'communicate',\n",
       " 'communicated',\n",
       " 'communication',\n",
       " 'companies',\n",
       " 'company',\n",
       " 'compared',\n",
       " 'competitive',\n",
       " 'complete',\n",
       " 'completed',\n",
       " 'completely',\n",
       " 'complicated',\n",
       " 'comps',\n",
       " 'concerns',\n",
       " 'confident',\n",
       " 'cons',\n",
       " 'considered',\n",
       " 'constant',\n",
       " 'contact',\n",
       " 'contacted',\n",
       " 'continue',\n",
       " 'continued',\n",
       " 'contract',\n",
       " 'conventional',\n",
       " 'conversation',\n",
       " 'conversations',\n",
       " 'cost',\n",
       " 'costs',\n",
       " 'could',\n",
       " 'couldn',\n",
       " 'couldnt',\n",
       " 'course',\n",
       " 'cover',\n",
       " 'credit',\n",
       " 'current',\n",
       " 'customer',\n",
       " 'customers',\n",
       " 'daily',\n",
       " 'dallas',\n",
       " 'date',\n",
       " 'day',\n",
       " 'days',\n",
       " 'deal',\n",
       " 'dealing',\n",
       " 'dealt',\n",
       " 'dean',\n",
       " 'decided',\n",
       " 'decision',\n",
       " 'definitely',\n",
       " 'delays',\n",
       " 'despite',\n",
       " 'detail',\n",
       " 'details',\n",
       " 'didnt',\n",
       " 'difference',\n",
       " 'different',\n",
       " 'difficult',\n",
       " 'directly',\n",
       " 'dlj',\n",
       " 'docs',\n",
       " 'documentation',\n",
       " 'documents',\n",
       " 'done',\n",
       " 'dont',\n",
       " 'dream',\n",
       " 'due',\n",
       " 'earlier',\n",
       " 'easier',\n",
       " 'easily',\n",
       " 'easy',\n",
       " 'effective',\n",
       " 'efficient',\n",
       " 'else',\n",
       " 'email',\n",
       " 'emailed',\n",
       " 'emails',\n",
       " 'encountered',\n",
       " 'end',\n",
       " 'ended',\n",
       " 'enough',\n",
       " 'ensure',\n",
       " 'entire',\n",
       " 'error',\n",
       " 'errors',\n",
       " 'escrow',\n",
       " 'especially',\n",
       " 'estate',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'exactly',\n",
       " 'exceeded',\n",
       " 'excellent',\n",
       " 'expect',\n",
       " 'expectations',\n",
       " 'expected',\n",
       " 'experience',\n",
       " 'experienced',\n",
       " 'experiences',\n",
       " 'expertise',\n",
       " 'expired',\n",
       " 'explain',\n",
       " 'explained',\n",
       " 'explaining',\n",
       " 'express',\n",
       " 'extra',\n",
       " 'extremely',\n",
       " 'fact',\n",
       " 'failed',\n",
       " 'fall',\n",
       " 'family',\n",
       " 'fantastic',\n",
       " 'far',\n",
       " 'fast',\n",
       " 'feel',\n",
       " 'fees',\n",
       " 'felt',\n",
       " 'final',\n",
       " 'finally',\n",
       " 'financial',\n",
       " 'financing',\n",
       " 'find',\n",
       " 'fine',\n",
       " 'finish',\n",
       " 'first',\n",
       " 'five',\n",
       " 'follow',\n",
       " 'following',\n",
       " 'fortunate',\n",
       " 'forward',\n",
       " 'found',\n",
       " 'four',\n",
       " 'fred',\n",
       " 'free',\n",
       " 'friend',\n",
       " 'friendly',\n",
       " 'friends',\n",
       " 'front',\n",
       " 'frustrating',\n",
       " 'full',\n",
       " 'fully',\n",
       " 'funded',\n",
       " 'future',\n",
       " 'g',\n",
       " 'gave',\n",
       " 'get',\n",
       " 'getting',\n",
       " 'give',\n",
       " 'given',\n",
       " 'go',\n",
       " 'goals',\n",
       " 'going',\n",
       " 'good',\n",
       " 'goodlet',\n",
       " 'goods',\n",
       " 'got',\n",
       " 'great',\n",
       " 'guaranteed',\n",
       " 'guidance',\n",
       " 'guide',\n",
       " 'guy',\n",
       " 'guys',\n",
       " 'handled',\n",
       " 'hands',\n",
       " 'happen',\n",
       " 'happened',\n",
       " 'happening',\n",
       " 'happier',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'heard',\n",
       " 'help',\n",
       " 'helped',\n",
       " 'helpful',\n",
       " 'helping',\n",
       " 'hesitate',\n",
       " 'high',\n",
       " 'higher',\n",
       " 'highly',\n",
       " 'hold',\n",
       " 'home',\n",
       " 'homebuyer',\n",
       " 'homes',\n",
       " 'honest',\n",
       " 'honestly',\n",
       " 'hope',\n",
       " 'horrible',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'household',\n",
       " 'houses',\n",
       " 'however',\n",
       " 'hung',\n",
       " 'husband',\n",
       " 'i',\n",
       " 'id',\n",
       " 'im',\n",
       " 'immediately',\n",
       " 'impressed',\n",
       " 'including',\n",
       " 'income',\n",
       " 'industry',\n",
       " 'information',\n",
       " 'informative',\n",
       " 'informed',\n",
       " 'initial',\n",
       " 'initially',\n",
       " 'institution',\n",
       " 'instructed',\n",
       " 'interest',\n",
       " 'interested',\n",
       " 'involved',\n",
       " 'issues',\n",
       " 'items',\n",
       " 'ive',\n",
       " 'jason',\n",
       " 'jeremy',\n",
       " 'job',\n",
       " 'jocovic',\n",
       " 'joey',\n",
       " 'jon',\n",
       " 'june',\n",
       " 'k',\n",
       " 'keep',\n",
       " 'kept',\n",
       " 'kind',\n",
       " 'kirk',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'knowing',\n",
       " 'knowledge',\n",
       " 'knowledgeable',\n",
       " 'known',\n",
       " 'kory',\n",
       " 'last',\n",
       " 'late',\n",
       " 'later',\n",
       " 'least',\n",
       " 'left',\n",
       " 'lender',\n",
       " 'lenders',\n",
       " 'lending',\n",
       " 'less',\n",
       " 'let',\n",
       " 'letter',\n",
       " 'life',\n",
       " 'like',\n",
       " 'line',\n",
       " 'list',\n",
       " 'little',\n",
       " 'll',\n",
       " 'loan',\n",
       " 'loans',\n",
       " 'local',\n",
       " 'lock',\n",
       " 'locked',\n",
       " 'long',\n",
       " 'longer',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'lot',\n",
       " 'lower',\n",
       " 'made',\n",
       " 'mail',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'making',\n",
       " 'man',\n",
       " 'manner',\n",
       " 'many',\n",
       " 'market',\n",
       " 'matt',\n",
       " 'matter',\n",
       " 'may',\n",
       " 'meet',\n",
       " 'mentioned',\n",
       " 'messages',\n",
       " 'met',\n",
       " 'mile',\n",
       " 'military',\n",
       " 'minimal',\n",
       " 'minutes',\n",
       " 'missed',\n",
       " 'mistake',\n",
       " 'money',\n",
       " 'month',\n",
       " 'monthly',\n",
       " 'months',\n",
       " 'mortgage',\n",
       " 'mortgages',\n",
       " 'mostly',\n",
       " 'move',\n",
       " 'moving',\n",
       " 'mr',\n",
       " 'much',\n",
       " 'multiple',\n",
       " 'name',\n",
       " 'nasb',\n",
       " 'nasbs',\n",
       " 'nc',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'needs',\n",
       " 'never',\n",
       " 'new',\n",
       " 'next',\n",
       " 'night',\n",
       " 'none',\n",
       " 'north',\n",
       " 'notary',\n",
       " 'note',\n",
       " 'nothing',\n",
       " 'number',\n",
       " 'offer',\n",
       " 'offered',\n",
       " 'offers',\n",
       " 'office',\n",
       " 'officer',\n",
       " 'ok',\n",
       " 'old',\n",
       " 'one',\n",
       " 'online',\n",
       " 'option',\n",
       " 'options',\n",
       " 'oriented',\n",
       " 'original',\n",
       " 'originally',\n",
       " 'others',\n",
       " 'outstanding',\n",
       " 'overall',\n",
       " 'pacific',\n",
       " 'paid',\n",
       " 'painless',\n",
       " 'paper',\n",
       " 'paperwork',\n",
       " 'part',\n",
       " 'passionate',\n",
       " 'past',\n",
       " 'patient',\n",
       " 'patiently',\n",
       " 'patrick',\n",
       " 'pay',\n",
       " 'paying',\n",
       " 'payment',\n",
       " 'payments',\n",
       " 'people',\n",
       " 'person',\n",
       " 'personal',\n",
       " 'personally',\n",
       " 'peter',\n",
       " 'phone',\n",
       " 'plan',\n",
       " 'planned',\n",
       " 'pleasant',\n",
       " 'please',\n",
       " 'pleased',\n",
       " 'pleasure',\n",
       " 'plus',\n",
       " 'point',\n",
       " 'poor',\n",
       " 'position',\n",
       " 'positive',\n",
       " 'possible',\n",
       " 'potential',\n",
       " 'preapproval',\n",
       " 'pretty',\n",
       " 'previous',\n",
       " 'problem',\n",
       " 'problems',\n",
       " 'process',\n",
       " 'processing',\n",
       " 'processor',\n",
       " 'product',\n",
       " 'professional',\n",
       " 'professionalism',\n",
       " 'professionally',\n",
       " 'promised',\n",
       " 'prompt',\n",
       " 'promptly',\n",
       " 'properties',\n",
       " 'property',\n",
       " 'pros',\n",
       " 'provide',\n",
       " 'provided',\n",
       " 'pulled',\n",
       " 'purchase',\n",
       " 'purchased',\n",
       " 'purchasing',\n",
       " 'put',\n",
       " 'question',\n",
       " 'questions',\n",
       " 'quick',\n",
       " 'quickly',\n",
       " 'quite',\n",
       " 'quotes',\n",
       " 'ran',\n",
       " 'rate',\n",
       " 'rates',\n",
       " 'reach',\n",
       " 'reached',\n",
       " 'read',\n",
       " 'ready',\n",
       " 'real',\n",
       " 'really',\n",
       " 'realtor',\n",
       " 'receive',\n",
       " 'received',\n",
       " 'recommend',\n",
       " 'recommended',\n",
       " 'record',\n",
       " 'refi',\n",
       " 'refinance',\n",
       " 'refinanced',\n",
       " 'refinancing',\n",
       " 'regulations',\n",
       " 'reliance',\n",
       " 'rep',\n",
       " 'representative',\n",
       " 'request',\n",
       " 'requested',\n",
       " 'requests',\n",
       " 'required',\n",
       " 'respond',\n",
       " 'responded',\n",
       " 'response',\n",
       " 'responses',\n",
       " 'responsive',\n",
       " 'responsiveness',\n",
       " 'result',\n",
       " 'returned',\n",
       " 'review',\n",
       " 'reviews',\n",
       " 'rick',\n",
       " 'right',\n",
       " 'robert',\n",
       " 'rude',\n",
       " 's',\n",
       " 'said',\n",
       " 'sale',\n",
       " 'satisfied',\n",
       " 'save',\n",
       " 'saved',\n",
       " 'savings',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'scheduled',\n",
       " 'score',\n",
       " 'second',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'selected',\n",
       " 'sell',\n",
       " 'send',\n",
       " 'sending',\n",
       " 'sent',\n",
       " 'service',\n",
       " 'services',\n",
       " 'set',\n",
       " 'several',\n",
       " 'show',\n",
       " 'showed',\n",
       " 'showing',\n",
       " 'side',\n",
       " 'sign',\n",
       " 'signed',\n",
       " 'simple',\n",
       " 'since',\n",
       " 'situation',\n",
       " 'skills',\n",
       " 'slow',\n",
       " 'small',\n",
       " 'smooth',\n",
       " 'smoothly',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'special',\n",
       " 'specifically',\n",
       " 'speed',\n",
       " 'spent',\n",
       " 'spoke',\n",
       " 'staff',\n",
       " 'stage',\n",
       " 'star',\n",
       " 'stars',\n",
       " 'start',\n",
       " 'started',\n",
       " 'state',\n",
       " 'status',\n",
       " 'stayed',\n",
       " 'steer',\n",
       " 'step',\n",
       " 'stephanie',\n",
       " 'steve',\n",
       " 'steven',\n",
       " 'still',\n",
       " 'stopped',\n",
       " 'stress',\n",
       " 'stressful',\n",
       " 'successful',\n",
       " 'suggested',\n",
       " 'super',\n",
       " 'support',\n",
       " 'sure',\n",
       " 'surprises',\n",
       " 'system',\n",
       " 't',\n",
       " 'take',\n",
       " 'taken',\n",
       " 'talk',\n",
       " 'talked',\n",
       " 'talking',\n",
       " 'team',\n",
       " 'tell',\n",
       " 'telling',\n",
       " 'terms',\n",
       " 'text',\n",
       " 'th',\n",
       " 'thank',\n",
       " 'thankful',\n",
       " 'thanks',\n",
       " 'thats',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'thorough',\n",
       " 'thoroughly',\n",
       " 'though',\n",
       " 'thought',\n",
       " 'thousands',\n",
       " 'three',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'time',\n",
       " 'timely',\n",
       " 'times',\n",
       " 'together',\n",
       " 'told',\n",
       " 'took',\n",
       " 'top',\n",
       " 'total',\n",
       " 'touch',\n",
       " 'track',\n",
       " 'transaction',\n",
       " 'tree',\n",
       " 'tried',\n",
       " 'triumph',\n",
       " 'true',\n",
       " 'truly',\n",
       " 'trusted',\n",
       " 'trying',\n",
       " 'twice',\n",
       " 'two',\n",
       " 'types',\n",
       " 'ultimately',\n",
       " 'understand',\n",
       " 'understood',\n",
       " 'unless',\n",
       " 'unlike',\n",
       " 'unpleasant',\n",
       " 'unprofessional',\n",
       " 'updates',\n",
       " 'upfront',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'using',\n",
       " 'va',\n",
       " 'vacation',\n",
       " 'various',\n",
       " 've',\n",
       " 'veteran',\n",
       " 'via',\n",
       " 'walk',\n",
       " 'walked',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'wasnt',\n",
       " 'waste',\n",
       " 'way',\n",
       " 'waymire',\n",
       " 'we',\n",
       " 'website',\n",
       " 'week',\n",
       " 'weekends',\n",
       " 'weeks',\n",
       " 'well',\n",
       " 'went',\n",
       " 'werent',\n",
       " 'weve',\n",
       " 'whatever',\n",
       " 'whole',\n",
       " 'wife',\n",
       " 'willing',\n",
       " 'within',\n",
       " 'without',\n",
       " 'woman',\n",
       " 'wonderful',\n",
       " 'word',\n",
       " 'work',\n",
       " 'worked',\n",
       " 'working',\n",
       " 'works',\n",
       " 'worth',\n",
       " 'would',\n",
       " 'wouldnt',\n",
       " 'writing',\n",
       " 'written',\n",
       " 'wrong',\n",
       " 'wyndham',\n",
       " 'year',\n",
       " 'years',\n",
       " 'yet']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm1=pd.DataFrame(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm1.columns=count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaron</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accept</th>\n",
       "      <th>accommodating</th>\n",
       "      <th>account</th>\n",
       "      <th>accurate</th>\n",
       "      <th>achieve</th>\n",
       "      <th>across</th>\n",
       "      <th>...</th>\n",
       "      <th>worth</th>\n",
       "      <th>would</th>\n",
       "      <th>wouldnt</th>\n",
       "      <th>writing</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>wyndham</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 732 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaron  ability  able  absolutely  accept  accommodating  account  accurate  \\\n",
       "0      0        0     0           0       1              0        0         0   \n",
       "1      0        0     0           0       0              0        0         0   \n",
       "2      0        0     0           0       0              0        0         0   \n",
       "3      0        0     0           0       1              0        0         0   \n",
       "4      0        0     0           0       0              0        0         0   \n",
       "\n",
       "   achieve  across  ...  worth  would  wouldnt  writing  written  wrong  \\\n",
       "0        0       0  ...      0      0        1        0        0      0   \n",
       "1        0       0  ...      0      0        0        0        0      0   \n",
       "2        0       0  ...      0      0        0        0        0      0   \n",
       "3        0       0  ...      0      0        1        0        0      0   \n",
       "4        0       0  ...      0      0        0        0        0      0   \n",
       "\n",
       "   wyndham  year  years  yet  \n",
       "0        0     0      1    0  \n",
       "1        0     0      0    0  \n",
       "2        0     0      0    0  \n",
       "3        0     0      1    0  \n",
       "4        0     0      0    0  \n",
       "\n",
       "[5 rows x 732 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization (count, tfidf) for both train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "count_vect = CountVectorizer(analyzer='word', \n",
    "                             token_pattern=r'\\w{1,}', \n",
    "                             ngram_range=(1, 1 ), \n",
    "                             min_df=5, \n",
    "                             encoding='latin-1' , \n",
    "                             max_features=800)\n",
    "\n",
    "xtrain_count = count_vect.fit_transform(X_train)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(xtrain_count)\n",
    "\n",
    "#Test\n",
    "#count_vect = CountVectorizer()\n",
    "xtest_count = count_vect.transform(X_test)\n",
    "\n",
    "#tfidf_transformer = TfidfTransformer()\n",
    "X_test_tfidf = tfidf_transformer.transform(xtest_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm2=pd.DataFrame(X_train_tfidf.toarray(), columns=count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaron</th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accept</th>\n",
       "      <th>accommodating</th>\n",
       "      <th>account</th>\n",
       "      <th>accurate</th>\n",
       "      <th>across</th>\n",
       "      <th>actual</th>\n",
       "      <th>adam</th>\n",
       "      <th>...</th>\n",
       "      <th>working</th>\n",
       "      <th>works</th>\n",
       "      <th>would</th>\n",
       "      <th>wouldnt</th>\n",
       "      <th>writing</th>\n",
       "      <th>wrong</th>\n",
       "      <th>wyndham</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096250</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194406</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121363</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.094403</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.084774</td>\n",
       "      <td>0.175643</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.256938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.228774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 596 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaron  able  absolutely    accept  accommodating  account  accurate  \\\n",
       "0    0.0   0.0    0.000000  0.133259            0.0      0.0  0.000000   \n",
       "1    0.0   0.0    0.000000  0.000000            0.0      0.0  0.000000   \n",
       "2    0.0   0.0    0.000000  0.000000            0.0      0.0  0.000000   \n",
       "3    0.0   0.0    0.000000  0.130702            0.0      0.0  0.000000   \n",
       "4    0.0   0.0    0.000000  0.000000            0.0      0.0  0.000000   \n",
       "5    0.0   0.0    0.000000  0.000000            0.0      0.0  0.000000   \n",
       "6    0.0   0.0    0.000000  0.000000            0.0      0.0  0.256938   \n",
       "7    0.0   0.0    0.000000  0.000000            0.0      0.0  0.000000   \n",
       "8    0.0   0.0    0.000000  0.000000            0.0      0.0  0.000000   \n",
       "9    0.0   0.0    0.228774  0.000000            0.0      0.0  0.000000   \n",
       "\n",
       "   across  actual  adam  ...   working  works     would   wouldnt  writing  \\\n",
       "0     0.0     0.0   0.0  ...  0.000000    0.0  0.000000  0.123737      0.0   \n",
       "1     0.0     0.0   0.0  ...  0.000000    0.0  0.000000  0.000000      0.0   \n",
       "2     0.0     0.0   0.0  ...  0.194406    0.0  0.000000  0.000000      0.0   \n",
       "3     0.0     0.0   0.0  ...  0.000000    0.0  0.000000  0.121363      0.0   \n",
       "4     0.0     0.0   0.0  ...  0.000000    0.0  0.000000  0.000000      0.0   \n",
       "5     0.0     0.0   0.0  ...  0.000000    0.0  0.084774  0.175643      0.0   \n",
       "6     0.0     0.0   0.0  ...  0.000000    0.0  0.000000  0.000000      0.0   \n",
       "7     0.0     0.0   0.0  ...  0.000000    0.0  0.068074  0.000000      0.0   \n",
       "8     0.0     0.0   0.0  ...  0.000000    0.0  0.000000  0.000000      0.0   \n",
       "9     0.0     0.0   0.0  ...  0.000000    0.0  0.000000  0.000000      0.0   \n",
       "\n",
       "   wrong  wyndham  year     years  yet  \n",
       "0    0.0      0.0   0.0  0.096250  0.0  \n",
       "1    0.0      0.0   0.0  0.000000  0.0  \n",
       "2    0.0      0.0   0.0  0.000000  0.0  \n",
       "3    0.0      0.0   0.0  0.094403  0.0  \n",
       "4    0.0      0.0   0.0  0.000000  0.0  \n",
       "5    0.0      0.0   0.0  0.000000  0.0  \n",
       "6    0.0      0.0   0.0  0.000000  0.0  \n",
       "7    0.0      0.0   0.0  0.000000  0.0  \n",
       "8    0.0      0.0   0.0  0.000000  0.0  \n",
       "9    0.0      0.0   0.0  0.000000  0.0  \n",
       "\n",
       "[10 rows x 596 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern='\\w{1,}', ngram_range=(1, 2), max_features=800)\n",
    "tfidf_vect_ngram.fit(df['Reviews'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "xtest_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create user defined function for train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid,  valid_y):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    return metrics.accuracy_score(classifier.predict(feature_vector_train), label), metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building different models with different vectors  (multiclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of unique values of the said array:\n",
      "[[  1   5]\n",
      " [ 70 308]]\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(y_train, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of unique values of the said array:\n",
      "[[  1   5]\n",
      " [308 308]]\n"
     ]
    }
   ],
   "source": [
    "#!pip install imblearn\n",
    "ros = RandomOverSampler(random_state=123)\n",
    "\n",
    "X_train_tfidf_os, y_train_tfidf_os = ros.fit_sample(X_train_tfidf, y_train)\n",
    "\n",
    "X_train_cnt_os, y_train_cnt_os = ros.fit_sample(xtrain_count, y_train)\n",
    "\n",
    "X_train_tfidf_ngram_os, y_train_tfidf_ngram_os = ros.fit_sample(xtrain_tfidf_ngram, y_train)\n",
    "\n",
    "unique_elements, counts_elements = np.unique(y_train_tfidf_os, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB  for L1, Count Vectors:  (0.9756493506493507, 0.952755905511811)\n",
      "NB  for L1, WordLevel TF-IDF:  (0.9724025974025974, 0.9606299212598425)\n",
      "NB  for L1, N-Gram Vectors:  (0.9675324675324676, 0.9448818897637795)\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "# Naive Bayes on TF-IDF\n",
    "accuracy_L1 = train_model(naive_bayes.MultinomialNB(), X_train_tfidf_os, y_train_tfidf_os, X_test_tfidf, y_test)\n",
    "print(\"NB  for L1, Count Vectors: \", accuracy_L1)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy_L1 = train_model(naive_bayes.MultinomialNB(), X_train_cnt_os, y_train_cnt_os, xtest_count, y_test)\n",
    "print(\"NB  for L1, WordLevel TF-IDF: \", accuracy_L1)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy_L1 = train_model(naive_bayes.MultinomialNB(), X_train_tfidf_ngram_os, y_train_tfidf_ngram_os, xtest_tfidf_ngram, y_test)\n",
    "print(\"NB  for L1, N-Gram Vectors: \", accuracy_L1)\n",
    "\n",
    "# gap between train and test is high so reject if its less we could use it\n",
    "#0.617335352006056 train score\n",
    "#0.4752 test score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR  for L1, Count Vectors:  (0.9837662337662337, 0.968503937007874)\n",
      "LR  for L1, WordLevel TF-IDF:  (0.9983766233766234, 0.9763779527559056)\n",
      "LR  for L1, N-Gram Vectors:  (0.9756493506493507, 0.9606299212598425)\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "# Logistic Regression on Count Vectors and TF-IDF\n",
    "accuracy_L1 = train_model(LogisticRegression(), X_train_tfidf_os, y_train_tfidf_os, X_test_tfidf, y_test)\n",
    "print(\"LR  for L1, Count Vectors: \", accuracy_L1)\n",
    "\n",
    "\n",
    "\n",
    "# Logistic Regression on Word Level TF IDF Vectors\n",
    "accuracy_L1 = train_model(LogisticRegression(), X_train_cnt_os, y_train_cnt_os, xtest_count, y_test)\n",
    "print(\"LR  for L1, WordLevel TF-IDF: \", accuracy_L1)\n",
    "\n",
    "\n",
    "\n",
    "# Logistic Regression on Ngram Level TF IDF Vectors\n",
    "accuracy_L1 = train_model(LogisticRegression(), X_train_tfidf_ngram_os, y_train_tfidf_ngram_os, xtest_tfidf_ngram, y_test)\n",
    "print(\"LR  for L1, N-Gram Vectors: \", accuracy_L1)\n",
    "\n",
    "# gap between train and test is high so reject if its less we could use it\n",
    "\n",
    "#0.6820590461771385 train score\n",
    "# 0.4672 test score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC  for L1, Count Vectors:  (0.9983766233766234, 0.937007874015748)\n",
      "SVC  for L1, WordLevel TF-IDF:  (0.9756493506493507, 0.9448818897637795)\n",
      "SVC  for L1, N-Gram Vectors:  (0.9983766233766234, 0.9448818897637795)\n"
     ]
    }
   ],
   "source": [
    "#Linear SVC\n",
    "# Linear SVC on Count Vectors and TF-IDF\n",
    "accuracy_L1 = train_model(SVC(), X_train_tfidf_os, y_train_tfidf_os, X_test_tfidf, y_test)\n",
    "print(\"SVC  for L1, Count Vectors: \", accuracy_L1)\n",
    "\n",
    "\n",
    "\n",
    "# Linear SVC on Word Level TF IDF Vectors\n",
    "accuracy_L1 = train_model(SVC(), X_train_cnt_os, y_train_cnt_os, xtest_count, y_test)\n",
    "print(\"SVC  for L1, WordLevel TF-IDF: \", accuracy_L1)\n",
    "\n",
    "\n",
    "\n",
    "# Linear SVC on Ngram Level TF IDF Vectors\n",
    "accuracy_L1 = train_model(SVC(), X_train_tfidf_ngram_os, y_train_tfidf_ngram_os, xtest_tfidf_ngram, y_test)\n",
    "print(\"SVC  for L1, N-Gram Vectors: \", accuracy_L1)\n",
    "\n",
    "# gap between train and test is high so reject if its less we could use it\n",
    "\n",
    "#0.5229371688115064 train score\n",
    "#0.3496 test score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Features to a Document-Term Matrix (binarly class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame that only contains the 5-star and 1-star reviews\n",
    "\n",
    "Bank['Stars'] = np.where(Bank['Stars']>3, 5, np.where(Bank['Stars']<3,1,3))\n",
    "\n",
    "Bank = Bank[(Bank.Stars==5) | (Bank.Stars==1)]\n",
    "\n",
    "# define X and y\n",
    "feature_cols = ['Reviews']\n",
    "X = Bank[feature_cols]\n",
    "y = Bank.Stars\n",
    "\n",
    "# split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(378, 627)\n",
      "(127, 627)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(378, 0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use CountVectorizer with text column only\n",
    "vect = TfidfVectorizer(lowercase=True, stop_words='english', max_features=1000, min_df=5, ngram_range=(1, 2))\n",
    "X_train_dtm = vect.fit_transform(X_train.Reviews)\n",
    "X_test_dtm = vect.transform(X_test.Reviews)\n",
    "print(X_train_dtm.shape)\n",
    "print(X_test_dtm.shape)\n",
    "\n",
    "# shape of other four feature columns\n",
    "X_train.drop('Reviews', axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(378, 2438)\n",
      "(127, 2438)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(378, 0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use CountVectorizer with text column only\n",
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train.Reviews)\n",
    "X_test_dtm = vect.transform(X_test.Reviews)\n",
    "print(X_train_dtm.shape)\n",
    "print(X_test_dtm.shape)\n",
    "\n",
    "# shape of other four feature columns\n",
    "X_train.drop('Reviews', axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127, 2438)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cast other feature columns to float and convert to a sparse matrix\n",
    "extra = sparse.csr_matrix(X_train.drop('Reviews', axis=1).astype(float))\n",
    "extra.shape\n",
    "\n",
    "# combine sparse matrices\n",
    "X_train_dtm_extra = sparse.hstack((X_train_dtm, extra))\n",
    "X_train_dtm_extra.shape\n",
    "\n",
    "# repeat for testing set\n",
    "extra = sparse.csr_matrix(X_test.drop('Reviews', axis=1).astype(float))\n",
    "X_test_dtm_extra = sparse.hstack((X_test_dtm, extra))\n",
    "X_test_dtm_extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.952755905511811\n",
      "0.9101960784313725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.952755905511811"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use logistic regression with text column only\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_dtm, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm)\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "print(metrics.roc_auc_score(y_test, y_pred_class))\n",
    "acc_log1=metrics.accuracy_score(y_test, y_pred_class)\n",
    "acc_log1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.952755905511811\n",
      "0.9101960784313725\n"
     ]
    }
   ],
   "source": [
    "# use logistic regression with all features\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_dtm_extra, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm_extra)\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "print(metrics.roc_auc_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.889763779527559"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use random forest with text column only\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rmf = RandomForestClassifier()\n",
    "rmf.fit(X_train_dtm, y_train)\n",
    "y_pred_class = rmf.predict(X_test_dtm)\n",
    "acc_rmf=metrics.accuracy_score(y_test, y_pred_class)\n",
    "acc_rmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.905511811023622"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use random forest with all features\n",
    "\n",
    "rmf = RandomForestClassifier()\n",
    "rmf.fit(X_train_dtm_extra, y_train)\n",
    "y_pred_class = rmf.predict(X_test_dtm_extra)\n",
    "acc_rmf1=metrics.accuracy_score(y_test, y_pred_class)\n",
    "acc_rmf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9212598425196851"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use gradient boosting with text column only\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc =  GradientBoostingClassifier()\n",
    "gbc.fit(X_train_dtm, y_train)\n",
    "y_pred_class = gbc.predict(X_test_dtm)\n",
    "acc_grad=metrics.accuracy_score(y_test, y_pred_class)\n",
    "acc_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9212598425196851"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use gradient boosting with all features\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(X_train_dtm_extra, y_train)\n",
    "y_pred_class = gbc.predict(X_test_dtm_extra)\n",
    "acc_grad1=metrics.accuracy_score(y_test, y_pred_class)\n",
    "acc_grad1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.952755905511811"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use Xgboosting with text column only\n",
    "from xgboost import XGBClassifier\n",
    "xgb =  XGBClassifier()\n",
    "xgb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = xgb.predict(X_test_dtm)\n",
    "acc_xgb=metrics.accuracy_score(y_test, y_pred_class)\n",
    "acc_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.952755905511811"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use Xgboosting with all features\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train_dtm_extra, y_train)\n",
    "y_pred_class = xgb.predict(X_test_dtm_extra)\n",
    "acc_xgb1=metrics.accuracy_score(y_test, y_pred_class)\n",
    "acc_xgb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9763779527559056"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use ANN with text column only\n",
    "mp = MLPClassifier()\n",
    "mp.fit(X_train_dtm, y_train)\n",
    "y_pred_class = mp.predict(X_test_dtm)\n",
    "acc_mlp=metrics.accuracy_score(y_test, y_pred_class)\n",
    "acc_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.968503937007874"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use ANN with all features\n",
    "mp = MLPClassifier()\n",
    "mp.fit(X_train_dtm_extra, y_train)\n",
    "y_pred_class = mp.predict(X_test_dtm_extra)\n",
    "acc_mlp1=metrics.accuracy_score(y_test, y_pred_class)\n",
    "acc_mlp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9448818897637795"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use ANN+logistic with text column only\n",
    "mp = MLPClassifier(activation='logistic')\n",
    "mp.fit(X_train_dtm, y_train)\n",
    "y_pred_class = mp.predict(X_test_dtm)\n",
    "acc_mlp_ln=metrics.accuracy_score(y_test, y_pred_class)\n",
    "acc_mlp_ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.952755905511811"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use ANN+logistic with all features\n",
    "mp = MLPClassifier(activation='logistic')\n",
    "mp.fit(X_train_dtm_extra, y_train)\n",
    "y_pred_class = mp.predict(X_test_dtm_extra)\n",
    "acc_mlp_ln1=metrics.accuracy_score(y_test, y_pred_class)\n",
    "acc_mlp_ln1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.952755905511811"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use OneVsRest + XGBoosting with text column only\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "one_xgb = OneVsRestClassifier(XGBClassifier())\n",
    "one_xgb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = one_xgb.predict(X_test_dtm)\n",
    "acc_one_xgb=metrics.accuracy_score(y_test, y_pred_class)\n",
    "acc_one_xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.952755905511811"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use OneVsRest + XGBoosting with all features\n",
    "one_xgb = OneVsRestClassifier(XGBClassifier())\n",
    "one_xgb.fit(X_train_dtm_extra, y_train)\n",
    "y_pred_class = one_xgb.predict(X_test_dtm_extra)\n",
    "acc_one_xgb1=metrics.accuracy_score(y_test, y_pred_class)\n",
    "acc_one_xgb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 16.0, 'Predicted label')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix( y_test, y_pred_class)\n",
    "sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [\"no\", \"Yes\"] , yticklabels = [\"No\", \"Yes\"] )\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compairing all modes with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.976378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.952756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.952756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OneVsRest + XGBoosting</td>\n",
       "      <td>0.952756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ANN + logistic</td>\n",
       "      <td>0.944882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoosting</td>\n",
       "      <td>0.921260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.889764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model     Score\n",
       "4                     ANN  0.976378\n",
       "0     Logistic Regression  0.952756\n",
       "3       Gradient Boosting  0.952756\n",
       "6  OneVsRest + XGBoosting  0.952756\n",
       "5          ANN + logistic  0.944882\n",
       "2              XGBoosting  0.921260\n",
       "1           Random Forest  0.889764"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression','Random Forest', 'XGBoosting','Gradient Boosting','ANN','ANN + logistic','OneVsRest + XGBoosting'],\n",
    "    \n",
    "    'Score': [acc_log1,acc_rmf,acc_grad,acc_xgb,acc_mlp,acc_mlp_ln,acc_one_xgb]\n",
    "    })\n",
    "\n",
    "models.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to additional TextBlob features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob: \"Simplified Text Processing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthew Richardson is professional and helpful. He helped us find the correct product for our mortgage. Thank you very much for the excellent service, Matthew!\n"
     ]
    }
   ],
   "source": [
    "print(Bank.Reviews[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great job, Wyndham Capital! Each person was professional and helped us move through our refinance process smoothly. Thank you!\n"
     ]
    }
   ],
   "source": [
    "# print the first review\n",
    "print(Bank.Reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it as a TextBlob object\n",
    "review = TextBlob(Bank.Reviews[0])\n",
    "#review1 = TextBlob('Good food, like it, excellent and recommended')\n",
    "#review1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5333333333333333"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_cmpkey', '_compare', '_create_sentence_objects', '_strkey', 'analyzer', 'classifier', 'classify', 'correct', 'detect_language', 'ends_with', 'endswith', 'find', 'format', 'index', 'join', 'json', 'lower', 'ngrams', 'noun_phrases', 'np_counts', 'np_extractor', 'parse', 'parser', 'polarity', 'pos_tagger', 'pos_tags', 'raw', 'raw_sentences', 'replace', 'rfind', 'rindex', 'sentences', 'sentiment', 'sentiment_assessments', 'serialized', 'split', 'starts_with', 'startswith', 'string', 'strip', 'stripped', 'subjectivity', 'tags', 'title', 'to_json', 'tokenize', 'tokenizer', 'tokens', 'translate', 'translator', 'upper', 'word_counts', 'words']\n"
     ]
    }
   ],
   "source": [
    "print(dir(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WordList(['Great']), WordList(['job']), WordList(['Wyndham']), WordList(['Capital']), WordList(['Each']), WordList(['person']), WordList(['was']), WordList(['professional']), WordList(['and']), WordList(['helped']), WordList(['us']), WordList(['move']), WordList(['through']), WordList(['our']), WordList(['refinance']), WordList(['process']), WordList(['smoothly']), WordList(['Thank']), WordList(['you'])]\n"
     ]
    }
   ],
   "source": [
    "print(review.ngrams(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5333333333333333"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Great', 'job', 'Wyndham', 'Capital', 'Each', 'person', 'was', 'professional', 'and', 'helped', 'us', 'move', 'through', 'our', 'refinance', 'process', 'smoothly', 'Thank', 'you'])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the words\n",
    "review.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"Great job, Wyndham Capital!\"),\n",
       " Sentence(\"Each person was professional and helped us move through our refinance process smoothly.\"),\n",
       " Sentence(\"Thank you!\")]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the sentences\n",
    "review.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"great job, wyndham capital! each person was professional and helped us move through our refinance process smoothly. thank you!\")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some string methods are available\n",
    "review.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Great', 'job']),\n",
       " WordList(['job', 'Wyndham']),\n",
       " WordList(['Wyndham', 'Capital']),\n",
       " WordList(['Capital', 'Each']),\n",
       " WordList(['Each', 'person']),\n",
       " WordList(['person', 'was']),\n",
       " WordList(['was', 'professional']),\n",
       " WordList(['professional', 'and']),\n",
       " WordList(['and', 'helped']),\n",
       " WordList(['helped', 'us']),\n",
       " WordList(['us', 'move']),\n",
       " WordList(['move', 'through']),\n",
       " WordList(['through', 'our']),\n",
       " WordList(['our', 'refinance']),\n",
       " WordList(['refinance', 'process']),\n",
       " WordList(['process', 'smoothly']),\n",
       " WordList(['smoothly', 'Thank']),\n",
       " WordList(['Thank', 'you'])]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.ngrams(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language correction, detection, translation etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"15 minutes late\")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spelling correction\n",
    "TextBlob('15 minuets late').correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=\"this is my email adcresc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"this is my email address\")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(s).correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('part', 0.9929478138222849), ('parrot', 0.007052186177715092)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spellcheck\n",
    "Word('parot').spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tip laterally',\n",
       " 'enclose with a bank',\n",
       " 'do business with a bank or keep an account at a bank',\n",
       " 'act as the banker in a game or in gambling',\n",
       " 'be in the banking business',\n",
       " 'put into a bank account',\n",
       " 'cover with ashes so to control the rate of burning',\n",
       " 'have confidence or faith in']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# definitions\n",
    "Word('bank').define('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'te'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# language detection\n",
    "TextBlob('టీమిండియా మాజీ ఓపెనర్‌ గౌతమ్ గంభీర్ మంగళవారం క్రికెట్‌కు రిటైర్‌మెంట్ ప్రకటించారు. దానిపై స్పందించిన బాలీవుడ్ స్టార్‌ షారుక్‌ ఖాన్‌ ఓ ట్వీట్ చేశారు').detect_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El ex abridor del Equipo India Gautam Gambhir anunció su retiro del cricket el martes En respuesta a esto la estrella de Bollywood Shah Rukh Khan tuiteó\n"
     ]
    }
   ],
   "source": [
    "# Language Translation\n",
    "a=' '.join(TextBlob('టీమిండియా మాజీ ఓపెనర్‌ గౌతమ్ గంభీర్ మంగళవారం క్రికెట్‌కు రిటైర్‌మెంట్ ప్రకటించారు. దానిపై స్పందించిన బాలీవుడ్ స్టార్‌ షారుక్‌ ఖాన్‌ ఓ ట్వీట్ చేశారు').translate(to='es').words)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming:**\n",
    "\n",
    "- **What:** Reduce a word to its base/stem/root form\n",
    "- **Why:** Often makes sense to treat related words the same way\n",
    "- **Notes:**\n",
    "    - Uses a \"simple\" and fast rule-based approach\n",
    "    - Stemmed words are usually not shown to users (used for analysis/indexing)\n",
    "    - Some search engines treat words with the same stem as synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.stem.snowball.SnowballStemmer at 0x18c272883c8>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize stemmer\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
    "stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Great', 'job', 'Wyndham', 'Capital', 'Each', 'person', 'was', 'professional', 'and', 'helped', 'us', 'move', 'through', 'our', 'refinance', 'process', 'smoothly', 'Thank', 'you'])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['great', 'job', 'wyndham', 'capit', 'each', 'person', 'was', 'profession', 'and', 'help', 'us', 'move', 'through', 'our', 'refin', 'process', 'smooth', 'thank', 'you']\n"
     ]
    }
   ],
   "source": [
    "# stem each word\n",
    "print([stemmer.stem(word) for word in review.words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**\n",
    "\n",
    "- **What:** Derive the canonical form ('lemma') of a word\n",
    "- **Why:** Can be better than stemming\n",
    "- **Notes:** Uses a dictionary-based approach (slower than stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Great', 'job', 'Wyndham', 'Capital', 'Each', 'person', 'was', 'professional', 'and', 'helped', 'us', 'move', 'through', 'our', 'refinance', 'process', 'smoothly', 'Thank', 'you'])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Great', 'job', 'Wyndham', 'Capital', 'Each', 'person', 'wa', 'professional', 'and', 'helped', 'u', 'move', 'through', 'our', 'refinance', 'process', 'smoothly', 'Thank', 'you']\n"
     ]
    }
   ],
   "source": [
    "# assume every word is a noun\n",
    "print([word.lemmatize() for word in review.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Great', 'job', 'Wyndham', 'Capital', 'Each', 'person', 'be', 'professional', 'and', 'help', 'us', 'move', 'through', 'our', 'refinance', 'process', 'smoothly', 'Thank', 'you']\n"
     ]
    }
   ],
   "source": [
    "# assume every word is a verb\n",
    "print([word.lemmatize(pos='v') for word in review.words])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
